openapi: 3.0.0
info:
  title: infiniti-Chat-LLama.cpp API
  version: '2.0.0'
  description: |
    REST API for interacting with LLama.cpp models. This API provides endpoints for model management,
    chat completions, and system monitoring.

servers:
  - url: http://localhost:8000
    description: Local development server
  - url: /api/v2
    description: UI server

tags:
  - name: Models
    description: Model management operations
  - name: Chat
    description: Chat completion endpoints
  - name: System
    description: System monitoring and health checks

paths:
  /models:
    get:
      summary: List Available Models
      description: Returns a list of all available models in the models directory and currently loaded models.
      operationId: listModels
      tags: [Models]
      responses:
        '200':
          description: List of available and loaded models
          content:
            application/json:
              schema:
                type: object
                properties:
                  available_models:
                    type: array
                    items:
                      $ref: '#/components/schemas/ModelInfo'
                  loaded_models:
                    type: object
                    additionalProperties:
                      $ref: '#/components/schemas/LoadedModelInfo'

  /models/{model_id}:
    get:
      summary: Get Model Information
      description: Retrieves detailed information about a specific model.
      operationId: getModelInfo
      tags: [Models]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID (filename) of the model
      responses:
        '200':
          description: Detailed model information
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DetailedModelInfo'

  /models/{model_id}/load:
    post:
      summary: Load Model
      description: Loads a model into memory with specified parameters.
      operationId: loadModel
      tags: [Models]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID (filename) of the model to load
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/LoadModelRequest'
      responses:
        '200':
          description: Model loaded successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/LoadModelResponse'

  /models/{model_id}/unload:
    post:
      summary: Unload Model
      description: Unloads a model from memory.
      operationId: unloadModel
      tags: [Models]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID (filename) of the model to unload
      responses:
        '200':
          description: Model unloaded successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/UnloadModelResponse'

  /chat/{model_id}:
    post:
      summary: Chat Completion
      description: Generate a chat completion with the specified model.
      operationId: chatCompletion
      tags: [Chat]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the model to use for chat
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatRequest'
      responses:
        '200':
          description: Chat completion response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatResponse'

  /chat/{model_id}/stream:
    post:
      summary: Chat Completion Stream
      description: Generate a streaming chat completion with the specified model.
      operationId: chatCompletionStream
      tags: [Chat]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the model to use for chat
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatRequest'
      responses:
        '200':
          description: Streaming chat completion response
          content:
            text/event-stream:
              schema:
                type: string
                description: Server-sent events stream of chat completion chunks

  /metrics:
    get:
      summary: System Metrics
      description: Get current system metrics including CPU, memory, and GPU usage.
      operationId: getMetrics
      tags: [System]
      responses:
        '200':
          description: Current system metrics
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SystemMetrics'

  /health:
    get:
      summary: Health Check
      description: Check the health status of the API server.
      operationId: healthCheck
      tags: [System]
      responses:
        '200':
          description: Health check response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthCheck'

components:
  schemas:
    ModelInfo:
      type: object
      properties:
        id:
          type: string
          description: Model identifier (filename)
        size:
          type: integer
          description: Size of the model file in bytes
        modified:
          type: string
          format: date-time
          description: Last modification timestamp
        metadata:
          type: object
          properties:
            model_type:
              type: string
              description: Type of the model (e.g., llama)
            parameters:
              type: object
              properties:
                context_length:
                  type: integer
                embedding_length:
                  type: integer
                block_count:
                  type: integer

    LoadedModelInfo:
      type: object
      properties:
        status:
          type: string
          enum: [loaded, loading, error]
        load_time:
          type: string
          format: date-time
        error:
          type: string

    DetailedModelInfo:
      allOf:
        - $ref: '#/components/schemas/ModelInfo'
        - type: object
          properties:
            status:
              type: string
              enum: [loaded, unloaded]
            performance:
              type: object
              properties:
                load_time:
                  type: string
                  format: date-time
                tokens_processed:
                  type: integer
                average_speed:
                  type: number

    LoadModelRequest:
      type: object
      properties:
        n_gpu_layers:
          type: integer
          description: Number of layers to offload to GPU
        n_ctx:
          type: integer
          description: Context window size
        n_batch:
          type: integer
          description: Batch size for prompt processing
        threads:
          type: integer
          description: Number of threads to use
        use_mlock:
          type: boolean
          description: Lock memory to prevent swapping
        f16_kv:
          type: boolean
          description: Use half-precision for key/value cache

    LoadModelResponse:
      type: object
      properties:
        status:
          type: string
          enum: [success, error]
        message:
          type: string
        load_time:
          type: string
          format: date-time
        memory_used:
          type: string

    UnloadModelResponse:
      type: object
      properties:
        status:
          type: string
          enum: [success, error]
        message:
          type: string
        unload_time:
          type: string
          format: date-time

    ChatRequest:
      type: object
      required: [messages]
      properties:
        messages:
          type: array
          items:
            type: object
            required: [role, content]
            properties:
              role:
                type: string
                enum: [system, user, assistant]
              content:
                type: string
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 0.7
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 0.95
        n_predict:
          type: integer
          minimum: 1
          default: 128

    ChatResponse:
      type: object
      properties:
        id:
          type: string
        object:
          type: string
        created:
          type: integer
        model:
          type: string
        choices:
          type: array
          items:
            type: object
            properties:
              index:
                type: integer
              message:
                type: object
                properties:
                  role:
                    type: string
                  content:
                    type: string
              finish_reason:
                type: string
        usage:
          type: object
          properties:
            prompt_tokens:
              type: integer
            completion_tokens:
              type: integer
            total_tokens:
              type: integer

    SystemMetrics:
      type: object
      properties:
        cpu:
          type: object
          properties:
            utilization:
              type: number
            cores:
              type: object
              properties:
                physical:
                  type: integer
                logical:
                  type: integer
            frequency:
              type: object
              properties:
                current:
                  type: integer
                min:
                  type: integer
                max:
                  type: integer
        memory:
          type: object
          properties:
            total_mb:
              type: integer
            available_mb:
              type: integer
            used_mb:
              type: integer
            percent:
              type: number
        gpu:
          type: object
          properties:
            available:
              type: boolean
            name:
              type: string
            memory:
              type: object
              properties:
                total_mb:
                  type: integer
                used_mb:
                  type: integer
                percent:
                  type: number
        timestamp:
          type: string
          format: date-time

    HealthCheck:
      type: object
      properties:
        status:
          type: string
          enum: [healthy, unhealthy]
        version:
          type: string
        uptime:
          type: string
        last_error:
          type: string 