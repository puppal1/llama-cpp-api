openapi: 3.0.0
info:
  title: infiniti-Chat-LLama.cpp API
  version: '2.0.0'
  description: |
    REST API for interacting with LLama.cpp models. This API provides endpoints for model management,
    chat completions, and system monitoring.

servers:
  - url: /api/v2
    description: API v2 Base URL
  - url: http://localhost:8000/api/v2
    description: Local development server

tags:
  - name: Models
    description: Model management operations
  - name: Chat
    description: Chat completion endpoints
  - name: System
    description: System monitoring and health checks

paths:
  /models:
    get:
      operationId: listModels
      summary: List Models
      description: |
        Returns a list of all available models, loaded models, and current memory metrics.
        Includes memory requirements and load safety status for each model.
      tags: [Models]
      responses:
        '200':
          description: List of available and loaded models with memory information
          content:
            application/json:
              schema:
                type: object
                properties:
                  available_models:
                    type: array
                    items:
                      $ref: '#/components/schemas/ModelInfo'
                  loaded_models:
                    type: array
                    items:
                      $ref: '#/components/schemas/ModelInfo'
                  memory_metrics:
                    $ref: '#/components/schemas/MemoryMetrics'
        '500':
          description: Internal server error
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Error'

  /models/{model_id}:
    get:
      summary: Get Model Information
      description: Retrieves detailed information about a specific model.
      operationId: getModelInfo
      tags: [Models]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID (filename) of the model
      responses:
        '200':
          description: Detailed model information
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/DetailedModelInfo'

  /models/{model_id}/load:
    post:
      operationId: loadModel
      summary: Load Model
      description: |
        Loads a model into memory with specified parameters.
        Performs memory requirement checks and validates context length.
        Will fail if:
        - Required memory exceeds available system memory
        - Context length exceeds maximum allowed (4096)
        - System doesn't have enough memory for safe operation
      tags: [Models]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the model to load
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/LoadModelRequest'
      responses:
        '200':
          description: Model loaded successfully
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/LoadModelResponse'
        '400':
          description: Memory requirements not met or invalid parameters
          content:
            application/json:
              schema:
                type: object
                properties:
                  detail:
                    type: string
                    description: Error message explaining why the model cannot be loaded
                    example: "Insufficient memory. Required: 12.5GB, Available: 8.2GB"

  /models/{model_id}/unload:
    post:
      operationId: unloadModel
      summary: Unload Model
      description: |
        Unloads a model from memory and releases its resources.
        Updates system memory metrics after unloading.
      tags: [Models]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the model to unload
      responses:
        '200':
          description: Model unloaded successfully
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    enum: [unloaded]
                  memory_metrics:
                    $ref: '#/components/schemas/MemoryMetrics'
        '404':
          description: Model not found or not loaded
          content:
            application/json:
              schema:
                type: object
                properties:
                  detail:
                    type: string

  /chat/{model_id}:
    post:
      summary: Chat Completion
      description: Generate a chat completion with the specified model.
      operationId: chatCompletion
      tags: [Chat]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the model to use for chat
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatRequest'
      responses:
        '200':
          description: Chat completion response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/ChatResponse'

  /chat/{model_id}/stream:
    post:
      summary: Chat Completion Stream
      description: Generate a streaming chat completion with the specified model.
      operationId: chatCompletionStream
      tags: [Chat]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the model to use for chat
      requestBody:
        required: true
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/ChatRequest'
      responses:
        '200':
          description: Streaming chat completion response
          content:
            text/event-stream:
              schema:
                type: string
                description: Server-sent events stream of chat completion chunks

  /chat/{model_id}/reset:
    post:
      summary: Reset Chat Context
      description: |
        Resets the chat context for the specified model, clearing KV cache and conversation history.
        This is useful when:
        1. You want to start a new conversation without model reload
        2. The context window is full and you want to clear it
        3. You want to free up memory from the KV cache
      operationId: resetChatContext
      tags: [Chat]
      parameters:
        - name: model_id
          in: path
          required: true
          schema:
            type: string
          description: The ID of the model whose chat context to reset
      responses:
        '200':
          description: Chat context reset successfully
          content:
            application/json:
              schema:
                type: object
                properties:
                  status:
                    type: string
                    enum: [reset]
                    description: Status of the reset operation
                  model_id:
                    type: string
                    description: ID of the model whose context was reset
                  memory_metrics:
                    $ref: '#/components/schemas/MemoryMetrics'
                    description: Updated memory metrics after context reset
        '404':
          description: Model not found or not loaded
          content:
            application/json:
              schema:
                type: object
                properties:
                  detail:
                    type: string

  /metrics:
    get:
      summary: System Metrics
      description: Get current system metrics including CPU, memory, and GPU usage.
      operationId: getMetrics
      tags: [System]
      responses:
        '200':
          description: Current system metrics
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/SystemMetrics'

  /health:
    get:
      summary: Health Check
      description: Check the health status of the API server.
      operationId: healthCheck
      tags: [System]
      responses:
        '200':
          description: Health check response
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/HealthCheck'

components:
  schemas:
    ModelInfo:
      type: object
      required: [id, metadata]
      properties:
        id:
          type: string
          description: Model identifier (filename)
        metadata:
          $ref: '#/components/schemas/ModelMetadata'

    ModelMetadata:
      type: object
      required: [model_type, size_gb, memory_required_gb, is_safe_to_load, status]
      properties:
        model_type:
          type: string
          description: Type/architecture of the model
          example: llama
        size_gb:
          type: number
          description: |
            Size of the model file in GB.
            Used as the base for memory requirement calculations.
        memory_required_gb:
          type: number
          description: |
            Estimated memory required to load the model, calculated as:
            base_memory = model_size_gb * 1.2 (development multiplier)
            context_scaling = max(0, (context_length - 2048) / 1024) * 0.25
            total_memory = base_memory * (1 + context_scaling)
            
            Example:
            - Model size: 4GB
            - Context: 4096 tokens
            - Base memory: 4GB * 1.2 = 4.8GB
            - Context scaling: ((4096 - 2048) / 1024) * 0.25 = 0.5
            - Total required: 4.8GB * (1 + 0.5) = 7.2GB
        is_safe_to_load:
          type: boolean
          description: |
            Whether the model can be safely loaded with current memory.
            Returns false if:
            1. required_memory > available_system_memory
            2. context_length > 4096 tokens
            3. available_memory < 2GB after loading
        current_memory_gb:
          type: number
          description: Current memory usage if model is loaded
        parameters:
          type: object
          properties:
            context_length:
              type: integer
              description: |
                Maximum context length supported.
                Every 1024 tokens above 2048 adds 25% to memory requirements.
            n_gpu_layers:
              type: integer
              description: Number of layers to offload to GPU
        status:
          type: string
          enum: [AVAILABLE, LOADING, LOADED, ERROR]
          description: |
            Current status of the model:
            - AVAILABLE: Model file exists but not loaded
            - LOADING: Model is currently being loaded
            - LOADED: Model is loaded and ready for use
            - ERROR: Error occurred during loading

    LoadModelRequest:
      type: object
      properties:
        n_ctx:
          type: integer
          description: |
            Context length for the model.
            IMPORTANT: Memory usage increases with context length:
            - Base context (2048): Uses base memory
            - Every +1024 tokens: Adds 25% more memory
            - Maximum allowed: 4096 tokens
            
            Examples:
            - 2048 tokens: Base memory
            - 3072 tokens: Base memory * 1.25
            - 4096 tokens: Base memory * 1.5
          minimum: 512
          maximum: 4096
          default: 2048
        n_gpu_layers:
          type: integer
          description: Number of layers to offload to GPU
          minimum: 0
          default: 0
        n_batch:
          type: integer
          description: Batch size for prompt processing
          minimum: 1
          default: 512
        threads:
          type: integer
          description: Number of threads to use
          minimum: 1
          default: 4
        use_mlock:
          type: boolean
          description: Whether to lock memory
          default: true
        f16_kv:
          type: boolean
          description: Use f16 key/value pairs
          default: true

    LoadModelResponse:
      type: object
      required: [status, model_id, memory_metrics]
      properties:
        status:
          type: string
          enum: [loaded, error]
        model_id:
          type: string
        info:
          type: object
          properties:
            load_time:
              type: string
              format: date-time
        memory_metrics:
          $ref: '#/components/schemas/MemoryMetrics'

    MemoryMetrics:
      type: object
      required: [system, models, total_model_memory_gb, available_for_models_gb]
      properties:
        system:
          type: object
          properties:
            total_gb:
              type: number
              description: Total system memory in GB
            available_gb:
              type: number
              description: Available system memory in GB
            used_gb:
              type: number
              description: Used system memory in GB
            percent_used:
              type: number
              description: Percentage of system memory used
        models:
          type: object
          additionalProperties:
            type: number
          description: Memory usage per loaded model in GB
        total_model_memory_gb:
          type: number
          description: Total memory used by all loaded models
        available_for_models_gb:
          type: number
          description: Memory available for loading new models

    DetailedModelInfo:
      allOf:
        - $ref: '#/components/schemas/ModelInfo'
        - type: object
          properties:
            status:
              type: string
              enum: [loaded, unloaded]
            performance:
              type: object
              properties:
                load_time:
                  type: string
                  format: date-time
                tokens_processed:
                  type: integer
                average_speed:
                  type: number

    ChatRequest:
      type: object
      required: [messages]
      properties:
        messages:
          type: array
          items:
            type: object
            required: [role, content]
            properties:
              role:
                type: string
                enum: [system, user, assistant]
              content:
                type: string
          minItems: 1
        temperature:
          type: number
          minimum: 0
          maximum: 2
          default: 0.7
        top_p:
          type: number
          minimum: 0
          maximum: 1
          default: 0.95
        stream:
          type: boolean
          default: false
        stop:
          oneOf:
            - type: string
            - type: array
              items:
                type: string
        max_tokens:
          type: integer
          minimum: 1
          default: 256

    ChatResponse:
      type: object
      required: [id, model, choices]
      properties:
        id:
          type: string
          description: Unique identifier for this chat completion
        model:
          type: string
          description: ID of the model used
        choices:
          type: array
          items:
            type: object
            required: [index, message]
            properties:
              index:
                type: integer
              message:
                type: object
                required: [role, content]
                properties:
                  role:
                    type: string
                    enum: [assistant]
                  content:
                    type: string
              finish_reason:
                type: string
                enum: [stop, length]
        usage:
          type: object
          properties:
            prompt_tokens:
              type: integer
            completion_tokens:
              type: integer
            total_tokens:
              type: integer

    SystemMetrics:
      type: object
      properties:
        cpu:
          type: object
          properties:
            utilization:
              type: number
            cores:
              type: object
              properties:
                physical:
                  type: integer
                logical:
                  type: integer
            frequency:
              type: object
              properties:
                current:
                  type: integer
                min:
                  type: integer
                max:
                  type: integer
        memory:
          type: object
          properties:
            total_mb:
              type: integer
            available_mb:
              type: integer
            used_mb:
              type: integer
            percent:
              type: number
        gpu:
          type: object
          properties:
            available:
              type: boolean
            name:
              type: string
            memory:
              type: object
              properties:
                total_mb:
                  type: integer
                used_mb:
                  type: integer
                percent:
                  type: number
        timestamp:
          type: string
          format: date-time

    HealthCheck:
      type: object
      properties:
        status:
          type: string
          enum: [healthy, unhealthy]
        version:
          type: string
        uptime:
          type: string
        last_error:
          type: string

    Error:
      type: object
      properties:
        detail:
          type: object
          properties:
            error:
              type: string
              description: Error type
              example: "Internal Server Error"
            message:
              type: string
              description: Detailed error message
              example: "Failed to load model metadata"
            type:
              type: string
              description: Python exception type
              example: "RuntimeError"
            traceback:
              type: string
              description: Full error traceback
            request_path:
              type: string
              description: Request path that caused the error
              example: "/api/v2/models"
            request_method:
              type: string
              description: HTTP method of the request
              example: "GET" 